{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review      Label  Fold\n",
      "0  James Chicago; the luxurious nice hotel as it ...  deceptive     1\n",
      "1  We booked a room at the Hilton Chicago for two...  deceptive     1\n",
      "2  I wish to express my dissatisfaction with my s...  deceptive     1\n",
      "3  Hotel is located 1/2 mile from the train stati...  deceptive     1\n",
      "4  I had high hopes for the Hilton Chicago, but I...  deceptive     1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path where your data is stored\n",
    "data_dir = 'op_spam_v1.4/negative_polarity'\n",
    "\n",
    "# Initialize empty lists to store data\n",
    "reviews = []\n",
    "labels = []\n",
    "folds = []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold in range(1, 6):\n",
    "    \n",
    "    # Read negative truthful reviews\n",
    "    truthful_dir = os.path.join(data_dir, 'deceptive_from_MTurk', f'fold{fold}')\n",
    "    for file in os.listdir(truthful_dir):\n",
    "        with open(os.path.join(truthful_dir, file), 'r', encoding='utf-8') as f:\n",
    "            review = f.read()\n",
    "            reviews.append(review)\n",
    "            labels.append('deceptive')\n",
    "            folds.append(fold)\n",
    "    \n",
    "    # Read negative deceptive reviews\n",
    "    deceptive_dir = os.path.join(data_dir, 'truthful_from_Web', f'fold{fold}')\n",
    "    for file in os.listdir(deceptive_dir):\n",
    "        with open(os.path.join(deceptive_dir, file), 'r', encoding='utf-8') as f:\n",
    "            review = f.read()\n",
    "            reviews.append(review)\n",
    "            labels.append('truthful')\n",
    "            folds.append(fold)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Review': reviews, 'Label': labels, 'Fold': folds})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder Classes:\n",
      "deceptive -> 0\n",
      "truthful -> 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Separate data for folds 1 to 4 (training) and fold 5 (testing)\n",
    "training_data = data[data['Fold'] != 5]\n",
    "testing_data = data[data['Fold'] == 5]\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels for training data\n",
    "y_train = label_encoder.fit_transform(training_data['Label'])\n",
    "\n",
    "# Transform the labels for testing data\n",
    "y_test = label_encoder.transform(testing_data['Label'])\n",
    "\n",
    "# Training data for folds 1 to 4\n",
    "X_train = training_data['Review']\n",
    "\n",
    "# Testing data for fold 5\n",
    "X_test = testing_data['Review']\n",
    "\n",
    "print(\"Label Encoder Classes:\")\n",
    "for original_label, encoded_label in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "    print(f\"{original_label} -> {encoded_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features as needed\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multinomial naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.96      0.81        80\n",
      "           1       0.94      0.60      0.73        80\n",
      "\n",
      "    accuracy                           0.78       160\n",
      "   macro avg       0.82      0.78      0.77       160\n",
      "weighted avg       0.82      0.78      0.77       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create and train the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_nB = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic regression with Lasso penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10}\n",
      "Accuracy: 0.84375\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        80\n",
      "           1       0.82      0.88      0.85        80\n",
      "\n",
      "    accuracy                           0.84       160\n",
      "   macro avg       0.85      0.84      0.84       160\n",
      "weighted avg       0.85      0.84      0.84       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)  # You can adjust the number of cross-validation folds\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', random_state=42, C=best_params['C'])\n",
    "best_logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_logistic_regression.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_lg = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'ccp_alpha': 0.01}\n",
      "Accuracy: 0.675\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.69        80\n",
      "           1       0.70      0.61      0.65        80\n",
      "\n",
      "    accuracy                           0.68       160\n",
      "   macro avg       0.68      0.68      0.67       160\n",
      "weighted avg       0.68      0.68      0.67       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'ccp_alpha': [0.001, 0.01, 0.1, 1]  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10)  # You can change the number of folds (cv) as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "best_decision_tree.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_decision_tree.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model and print results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_ct = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_features': 'log2', 'n_estimators': 300}\n",
      "Accuracy: 0.8375\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84        80\n",
      "           1       0.84      0.84      0.84        80\n",
      "\n",
      "    accuracy                           0.84       160\n",
      "   macro avg       0.84      0.84      0.84       160\n",
      "weighted avg       0.84      0.84      0.84       160\n",
      "\n",
      "Out-of-Bag Score: 0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42, oob_score=True)\n",
    "\n",
    "# Define hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Adjust these values as needed\n",
    "    'max_features': ['sqrt', 'log2']  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10)  # You can change the number of folds (cv) as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "best_random_forest.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_random_forest.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model and print results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Out-of-Bag Score:\", best_random_forest.oob_score_)\n",
    "\n",
    "pred_rf = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse vragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How does the performance of the generative linear model (multinomial naive Bayes) compare to the discriminative linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Calculate ROC curve and AUC for Logistic Regression\n",
    "logistic_regression_probs = best_logistic_regression.predict_proba(X_test_tfidf)[:, 1]\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, logistic_regression_probs)\n",
    "auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "# Calculate ROC curve and AUC for Naive Bayes\n",
    "nb_probs = nb_classifier.predict_proba(X_test_tfidf)[:, 1]\n",
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, nb_probs)\n",
    "auc_nb = auc(fpr_nb, tpr_nb)\n",
    "\n",
    "# Plot the ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.2f})')\n",
    "plt.plot(fpr_nb, tpr_nb, label=f'Multinomial Naive Bayes (AUC = {auc_nb:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is the Random Forest able to improve on the performance of the linear classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.84\n",
      "Multinomial Naive Bayes Accuracy: 0.78\n",
      "Random Forest Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for Logistic Regression\n",
    "y_pred_lr = best_logistic_regression.predict(X_test_tfidf)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Calculate accuracy for Multinomial Naive Bayes\n",
    "y_pred_nb = nb_classifier.predict(X_test_tfidf)\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# Calculate accuracy for Random Forest\n",
    "y_pred_rf = best_random_forest.predict(X_test_tfidf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Logistic Regression Accuracy: {accuracy_lr:.2f}')\n",
    "print(f'Multinomial Naive Bayes Accuracy: {accuracy_nb:.2f}')\n",
    "print(f'Random Forest Accuracy: {accuracy_rf:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does performance improve by adding bigram features, instead of using just unigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer with bigram features\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform the training data with bigram features\n",
    "X_train_tfidf_bigram = tfidf_vectorizer_bigram.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data with bigram features\n",
    "X_test_tfidf_bigram = tfidf_vectorizer_bigram.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multinomial naive Bayes BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8625\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87        80\n",
      "           1       0.88      0.84      0.86        80\n",
      "\n",
      "    accuracy                           0.86       160\n",
      "   macro avg       0.86      0.86      0.86       160\n",
      "weighted avg       0.86      0.86      0.86       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create and train the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf_bigram)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_nB_bigram = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic regression with Lasso penalty BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 100}\n",
      "Accuracy: 0.75625\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.71      0.75        80\n",
      "           1       0.74      0.80      0.77        80\n",
      "\n",
      "    accuracy                           0.76       160\n",
      "   macro avg       0.76      0.76      0.76       160\n",
      "weighted avg       0.76      0.76      0.76       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)  # You can adjust the number of cross-validation folds\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', random_state=42, C=best_params['C'])\n",
    "best_logistic_regression.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_logistic_regression.predict(X_test_tfidf_bigram)\n",
    "\n",
    "# Calculate and print the accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_lg_bigram = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification trees BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'ccp_alpha': 0.01}\n",
      "Accuracy: 0.6125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.45      0.54        80\n",
      "           1       0.58      0.78      0.67        80\n",
      "\n",
      "    accuracy                           0.61       160\n",
      "   macro avg       0.63      0.61      0.60       160\n",
      "weighted avg       0.63      0.61      0.60       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'ccp_alpha': [0.001, 0.01, 0.1, 1]  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10)  # You can change the number of folds (cv) as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_decision_tree = grid_search.best_estimator_\n",
    "best_decision_tree.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_decision_tree.predict(X_test_tfidf_bigram)\n",
    "\n",
    "# Evaluate the model and print results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "pred_ct_bigram = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random forests BIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_features': 'log2', 'n_estimators': 300}\n",
      "Accuracy: 0.83125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83        80\n",
      "           1       0.81      0.86      0.84        80\n",
      "\n",
      "    accuracy                           0.83       160\n",
      "   macro avg       0.83      0.83      0.83       160\n",
      "weighted avg       0.83      0.83      0.83       160\n",
      "\n",
      "Out-of-Bag Score: 0.8234375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest model\n",
    "random_forest = RandomForestClassifier(random_state=42, oob_score=True)\n",
    "\n",
    "# Define hyperparameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Adjust these values as needed\n",
    "    'max_features': ['sqrt', 'log2']  # Adjust these values as needed\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10)  # You can change the number of folds (cv) as needed\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit the final model with the best hyperparameters on all training data\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "best_random_forest.fit(X_train_tfidf_bigram, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_random_forest.predict(X_test_tfidf_bigram)\n",
    "\n",
    "# Evaluate the model and print results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"Out-of-Bag Score:\", best_random_forest.oob_score_)\n",
    "\n",
    "pred_rf_bigram = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the five most important terms (features) pointing towards a fake review? \n",
    "### What are the five most important terms (features) pointing towards a genuine review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients (weights) from the logistic regression model\n",
    "coefficients = best_logistic_regression.coef_\n",
    "\n",
    "# Get the feature names (e.g., words or terms) from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer_bigram.get_feature_names_out()\n",
    "\n",
    "# Create lists to store features associated with each class\n",
    "class_0_features = []\n",
    "class_1_features = []\n",
    "\n",
    "# Iterate through the coefficients and feature names\n",
    "for coef, feature in zip(coefficients[0], feature_names):\n",
    "    if coef < 0:  # Negative coefficients are associated with class 0 (genuine reviews)\n",
    "        class_0_features.append((feature, -coef))  # Use -coef to make it positive\n",
    "    else:\n",
    "        class_1_features.append((feature, coef))\n",
    "\n",
    "# Sort the features by the absolute value of their coefficients\n",
    "class_0_features.sort(key=lambda x: x[1], reverse=True)\n",
    "class_1_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top features for each class\n",
    "print(\"Top features for class 0 (genuine reviews):\")\n",
    "for feature, coef in class_0_features[:5]:  # Change the number to get more or fewer features\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(\"\\nTop features for class 1 (fake reviews):\")\n",
    "for feature, coef in class_1_features[:5]:  # Change the number to get more or fewer features\n",
    "    print(f\"{feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "feature_coefficients_mult = dict(zip(features, nb_classifier.feature_log_prob_[0]))\n",
    "feature_coefficients_lg = dict(zip(features, best_logistic_regression.coef_[0]))\n",
    "feature_coefficients_dt = dict(zip(features, best_decision_tree.feature_importances_))\n",
    "feature_coefficients_rf = dict(zip(features, best_random_forest.feature_importances_))\n",
    "\n",
    "top_fake_terms_mult = sorted(feature_coefficients_mult.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_fake_terms_lg = sorted(feature_coefficients_lg.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_fake_terms_dt = sorted(feature_coefficients_dt.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_fake_terms_rf = sorted(feature_coefficients_rf.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "top_true_terms_mult = sorted(feature_coefficients_mult.items(), key=lambda x: x[1], reverse=False)[:5]\n",
    "top_true_terms_lg = sorted(feature_coefficients_lg.items(), key=lambda x: x[1], reverse=False)[:5]\n",
    "top_true_terms_dt = sorted(feature_coefficients_dt.items(), key=lambda x: x[1], reverse=False)[:5]\n",
    "top_true_terms_rf = sorted(feature_coefficients_rf.items(), key=lambda x: x[1], reverse=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOP FAKE TERMS\\n')\n",
    "      \n",
    "print(f'Multinomial nB:\\n{[x[0] for x in top_fake_terms_mult]}\\n')\n",
    "print(f'Logstic regression:\\n{[x[0] for x in top_fake_terms_lg]}\\n')\n",
    "print(f'Classification tree:\\n{[x[0] for x in top_fake_terms_dt]}\\n')\n",
    "print(f'Random forest:\\n{[x[0] for x in top_fake_terms_rf]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOP GENUINE TERMS\\n')\n",
    "      \n",
    "print(f'Multinomial nB:\\n{[x[0] for x in top_true_terms_mult]}\\n')\n",
    "print(f'Logstic regression:\\n{[x[0] for x in top_true_terms_lg]}\\n')\n",
    "print(f'Classification tree:\\n{[x[0] for x in top_true_terms_dt]}\\n')\n",
    "print(f'Random forest:\\n{[x[0] for x in top_true_terms_rf]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Convert to np arrays for efficient computations\n",
    "pred_nB = np.array(pred_nB)\n",
    "pred_lg = np.array(pred_lg)\n",
    "pred_ct = np.array(pred_ct)\n",
    "pred_rf = np.array(pred_rf)\n",
    "\n",
    "pred_nB_bigram = np.array(pred_nB_bigram)\n",
    "pred_lg_bigram = np.array(pred_lg_bigram)\n",
    "pred_ct_bigram = np.array(pred_ct_bigram)\n",
    "pred_rf_bigram = np.array(pred_rf_bigram)\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative vs Discriminative (nB vs logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 113\n",
      "b: 12\n",
      "c: 22\n",
      "d: 13\n",
      "McNemar Chi-Square Statistic: 13.714285714285714\n",
      "P-value: 0.00021282941901437717\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_nB == y_test) & (pred_lg == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_nB == y_test) & (pred_lg != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_nB != y_test) & (pred_lg == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_nB != y_test) & (pred_lg != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest vs. linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 113\n",
      "b: 21\n",
      "c: 12\n",
      "d: 14\n",
      "McNemar Chi-Square Statistic: 16.40150893882237\n",
      "P-value: 5.1244419206563755e-05\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_rf == y_test) & (pred_nB == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_rf == y_test) & (pred_nB != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_rf != y_test) & (pred_nB == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_rf != y_test) & (pred_nB != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 121\n",
      "b: 13\n",
      "c: 14\n",
      "d: 12\n",
      "McNemar Chi-Square Statistic: 19.269124463154313\n",
      "P-value: 1.1352745479056956e-05\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_rf == y_test) & (pred_lg == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_rf == y_test) & (pred_lg != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_rf != y_test) & (pred_lg == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_rf != y_test) & (pred_lg != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram vs. Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 119\n",
      "b: 6\n",
      "c: 19\n",
      "d: 16\n",
      "McNemar Chi-Square Statistic: 35.22348955392434\n",
      "P-value: 2.9395648821751136e-09\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_nB == y_test) & (pred_nB_bigram == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_nB == y_test) & (pred_nB_bigram != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_nB != y_test) & (pred_nB_bigram == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_nB != y_test) & (pred_nB_bigram != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 119\n",
      "b: 16\n",
      "c: 2\n",
      "d: 23\n",
      "McNemar Chi-Square Statistic: 69.22370558734195\n",
      "P-value: 8.790510277839382e-17\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_lg == y_test) & (pred_lg_bigram == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_lg == y_test) & (pred_lg_bigram != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_lg != y_test) & (pred_lg_bigram == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_lg != y_test) & (pred_lg_bigram != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 77\n",
      "b: 31\n",
      "c: 21\n",
      "d: 31\n",
      "McNemar Chi-Square Statistic: 12.858662075251939\n",
      "P-value: 0.0003359206779047179\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_ct == y_test) & (pred_ct_bigram == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_ct == y_test) & (pred_ct_bigram != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_ct != y_test) & (pred_ct_bigram == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_ct != y_test) & (pred_ct_bigram != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 121\n",
      "b: 13\n",
      "c: 12\n",
      "d: 14\n",
      "McNemar Chi-Square Statistic: 27.185760037291853\n",
      "P-value: 1.8481490480307533e-07\n",
      "Reject the null hypothesis: There is a significant difference between the two classifiers.\n"
     ]
    }
   ],
   "source": [
    "# Calculate four possible options for evaluating two classifiers using McNemar test\n",
    "a = np.sum((pred_rf == y_test) & (pred_rf_bigram == y_test)) # single tree correct, bagged trees correct\n",
    "b = np.sum((pred_rf == y_test) & (pred_rf_bigram != y_test)) # single tree correct, bagged trees incorrect\n",
    "c = np.sum((pred_rf != y_test) & (pred_rf_bigram == y_test)) # single tree incorrect, bagged trees correct\n",
    "d = np.sum((pred_rf != y_test) & (pred_rf_bigram != y_test)) # single tree incorrect, bagged trees incorrect\n",
    "\n",
    "# Perform statistical test (McNemar)\n",
    "contingency_table = np.array([[a, b], [c, d]])\n",
    "chi2, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "# Print results\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"McNemar Chi-Square Statistic:\", chi2)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "# Evaluate statistical significance\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
